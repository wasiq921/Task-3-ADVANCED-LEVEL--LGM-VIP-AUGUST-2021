{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task # 2 (ADVANCED LEVEL TASK) - LGM VIP AUGUST 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvWsLvt9AfQlE3ZfC7cJwq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasiq921/Task-3-ADVANCED-LEVEL--LGM-VIP-AUGUST-2021/blob/main/Task_2_(ADVANCED_LEVEL_TASK)_LGM_VIP_AUGUST_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkTW-4Syq315"
      },
      "source": [
        "# Task 2 Next Word Prediction\n",
        "\n",
        "### Using Tensorflow and Keras library train a RNN , to predict the next word. \n",
        "#### Dataset Link: https://drive.google.com/file/d/1GeUzNVqiixXHnTl8oNiQ2W3CynX_lsu2/view\n",
        "\n",
        "## Author: Muhammad Wasiq\n",
        "##LGM VIP AUGUST 2021\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQs5dvSVneal"
      },
      "source": [
        "#import all the required libraries we need for this task\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCqAjKCRrVIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4581a1e9-3b6f-4068-e4c6-371f53b1e032"
      },
      "source": [
        "#loading the Dataset\n",
        "path = '1661-0.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 581888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JseRdm3D0rgN"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq0YorPM0-Ox",
        "outputId": "d8532be0-69a9-41d5-8be6-13681afc9baa"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project',\n",
              " 'gutenberg',\n",
              " 's',\n",
              " 'the',\n",
              " 'adventures',\n",
              " 'of',\n",
              " 'sherlock',\n",
              " 'holmes',\n",
              " 'by',\n",
              " 'arthur',\n",
              " 'conan',\n",
              " 'doyle',\n",
              " 'this',\n",
              " 'ebook',\n",
              " 'is',\n",
              " 'for',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'anyone',\n",
              " 'anywhere',\n",
              " 'at',\n",
              " 'no',\n",
              " 'cost',\n",
              " 'and',\n",
              " 'with',\n",
              " 'almost',\n",
              " 'no',\n",
              " 'restrictions',\n",
              " 'whatsoever',\n",
              " 'you',\n",
              " 'may',\n",
              " 'copy',\n",
              " 'it',\n",
              " 'give',\n",
              " 'it',\n",
              " 'away',\n",
              " 'or',\n",
              " 're',\n",
              " 'use',\n",
              " 'it',\n",
              " 'under',\n",
              " 'the',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'the',\n",
              " 'project',\n",
              " 'gutenberg',\n",
              " 'license',\n",
              " 'included',\n",
              " 'with',\n",
              " 'this',\n",
              " 'ebook',\n",
              " 'or',\n",
              " 'online',\n",
              " 'at',\n",
              " 'www',\n",
              " 'gutenberg',\n",
              " 'net',\n",
              " 'title',\n",
              " 'the',\n",
              " 'adventures',\n",
              " 'of',\n",
              " 'sherlock',\n",
              " 'holmes',\n",
              " 'author',\n",
              " 'arthur',\n",
              " 'conan',\n",
              " 'doyle',\n",
              " 'release',\n",
              " 'date',\n",
              " 'november',\n",
              " '29',\n",
              " '2002',\n",
              " 'ebook',\n",
              " '1661',\n",
              " 'last',\n",
              " 'updated',\n",
              " 'may',\n",
              " '20',\n",
              " '2019',\n",
              " 'language',\n",
              " 'english',\n",
              " 'character',\n",
              " 'set',\n",
              " 'encoding',\n",
              " 'utf',\n",
              " '8',\n",
              " 'start',\n",
              " 'of',\n",
              " 'this',\n",
              " 'project',\n",
              " 'gutenberg',\n",
              " 'ebook',\n",
              " 'the',\n",
              " 'adventures',\n",
              " 'of',\n",
              " 'sherlock',\n",
              " 'holmes',\n",
              " 'produced',\n",
              " 'by',\n",
              " 'an',\n",
              " 'anonymous',\n",
              " 'project',\n",
              " 'gutenberg',\n",
              " 'volunteer',\n",
              " 'and',\n",
              " 'jose',\n",
              " 'menendez',\n",
              " 'cover',\n",
              " 'the',\n",
              " 'adventures',\n",
              " 'of',\n",
              " 'sherlock',\n",
              " 'holmes',\n",
              " 'by',\n",
              " 'arthur',\n",
              " 'conan',\n",
              " 'doyle',\n",
              " 'contents',\n",
              " 'i',\n",
              " 'a',\n",
              " 'scandal',\n",
              " 'in',\n",
              " 'bohemia',\n",
              " 'ii',\n",
              " 'the',\n",
              " 'red',\n",
              " 'headed',\n",
              " 'league',\n",
              " 'iii',\n",
              " 'a',\n",
              " 'case',\n",
              " 'of',\n",
              " 'identity',\n",
              " 'iv',\n",
              " 'the',\n",
              " 'boscombe',\n",
              " 'valley',\n",
              " 'mystery',\n",
              " 'v',\n",
              " 'the',\n",
              " 'five',\n",
              " 'orange',\n",
              " 'pips',\n",
              " 'vi',\n",
              " 'the',\n",
              " 'man',\n",
              " 'with',\n",
              " 'the',\n",
              " 'twisted',\n",
              " 'lip',\n",
              " 'vii',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'blue',\n",
              " 'carbuncle',\n",
              " 'viii',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'speckled',\n",
              " 'band',\n",
              " 'ix',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'engineer',\n",
              " 's',\n",
              " 'thumb',\n",
              " 'x',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'noble',\n",
              " 'bachelor',\n",
              " 'xi',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'beryl',\n",
              " 'coronet',\n",
              " 'xii',\n",
              " 'the',\n",
              " 'adventure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'copper',\n",
              " 'beeches',\n",
              " 'i',\n",
              " 'a',\n",
              " 'scandal',\n",
              " 'in',\n",
              " 'bohemia',\n",
              " 'i',\n",
              " 'to',\n",
              " 'sherlock',\n",
              " 'holmes',\n",
              " 'she',\n",
              " 'is',\n",
              " 'always',\n",
              " '_the_',\n",
              " 'woman',\n",
              " 'i',\n",
              " 'have',\n",
              " 'seldom',\n",
              " 'heard',\n",
              " 'him',\n",
              " 'mention',\n",
              " 'her',\n",
              " 'under',\n",
              " 'any',\n",
              " 'other',\n",
              " 'name',\n",
              " 'in',\n",
              " 'his',\n",
              " 'eyes',\n",
              " 'she',\n",
              " 'eclipses',\n",
              " 'and',\n",
              " 'predominates',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'of',\n",
              " 'her',\n",
              " 'sex',\n",
              " 'it',\n",
              " 'was',\n",
              " 'not',\n",
              " 'that',\n",
              " 'he',\n",
              " 'felt',\n",
              " 'any',\n",
              " 'emotion',\n",
              " 'akin',\n",
              " 'to',\n",
              " 'love',\n",
              " 'for',\n",
              " 'irene',\n",
              " 'adler',\n",
              " 'all',\n",
              " 'emotions',\n",
              " 'and',\n",
              " 'that',\n",
              " 'one',\n",
              " 'particularly',\n",
              " 'were',\n",
              " 'abhorrent',\n",
              " 'to',\n",
              " 'his',\n",
              " 'cold',\n",
              " 'precise',\n",
              " 'but',\n",
              " 'admirably',\n",
              " 'balanced',\n",
              " 'mind',\n",
              " 'he',\n",
              " 'was',\n",
              " 'i',\n",
              " 'take',\n",
              " 'it',\n",
              " 'the',\n",
              " 'most',\n",
              " 'perfect',\n",
              " 'reasoning',\n",
              " 'and',\n",
              " 'observing',\n",
              " 'machine',\n",
              " 'that',\n",
              " 'the',\n",
              " 'world',\n",
              " 'has',\n",
              " 'seen',\n",
              " 'but',\n",
              " 'as',\n",
              " 'a',\n",
              " 'lover',\n",
              " 'he',\n",
              " 'would',\n",
              " 'have',\n",
              " 'placed',\n",
              " 'himself',\n",
              " 'in',\n",
              " 'a',\n",
              " 'false',\n",
              " 'position',\n",
              " 'he',\n",
              " 'never',\n",
              " 'spoke',\n",
              " 'of',\n",
              " 'the',\n",
              " 'softer',\n",
              " 'passions',\n",
              " 'save',\n",
              " 'with',\n",
              " 'a',\n",
              " 'gibe',\n",
              " 'and',\n",
              " 'a',\n",
              " 'sneer',\n",
              " 'they',\n",
              " 'were',\n",
              " 'admirable',\n",
              " 'things',\n",
              " 'for',\n",
              " 'the',\n",
              " 'observer',\n",
              " 'excellent',\n",
              " 'for',\n",
              " 'drawing',\n",
              " 'the',\n",
              " 'veil',\n",
              " 'from',\n",
              " 'men',\n",
              " 's',\n",
              " 'motives',\n",
              " 'and',\n",
              " 'actions',\n",
              " 'but',\n",
              " 'for',\n",
              " 'the',\n",
              " 'trained',\n",
              " 'reasoner',\n",
              " 'to',\n",
              " 'admit',\n",
              " 'such',\n",
              " 'intrusions',\n",
              " 'into',\n",
              " 'his',\n",
              " 'own',\n",
              " 'delicate',\n",
              " 'and',\n",
              " 'finely',\n",
              " 'adjusted',\n",
              " 'temperament',\n",
              " 'was',\n",
              " 'to',\n",
              " 'introduce',\n",
              " 'a',\n",
              " 'distracting',\n",
              " 'factor',\n",
              " 'which',\n",
              " 'might',\n",
              " 'throw',\n",
              " 'a',\n",
              " 'doubt',\n",
              " 'upon',\n",
              " 'all',\n",
              " 'his',\n",
              " 'mental',\n",
              " 'results',\n",
              " 'grit',\n",
              " 'in',\n",
              " 'a',\n",
              " 'sensitive',\n",
              " 'instrument',\n",
              " 'or',\n",
              " 'a',\n",
              " 'crack',\n",
              " 'in',\n",
              " 'one',\n",
              " 'of',\n",
              " 'his',\n",
              " 'own',\n",
              " 'high',\n",
              " 'power',\n",
              " 'lenses',\n",
              " 'would',\n",
              " 'not',\n",
              " 'be',\n",
              " 'more',\n",
              " 'disturbing',\n",
              " 'than',\n",
              " 'a',\n",
              " 'strong',\n",
              " 'emotion',\n",
              " 'in',\n",
              " 'a',\n",
              " 'nature',\n",
              " 'such',\n",
              " 'as',\n",
              " 'his',\n",
              " 'and',\n",
              " 'yet',\n",
              " 'there',\n",
              " 'was',\n",
              " 'but',\n",
              " 'one',\n",
              " 'woman',\n",
              " 'to',\n",
              " 'him',\n",
              " 'and',\n",
              " 'that',\n",
              " 'woman',\n",
              " 'was',\n",
              " 'the',\n",
              " 'late',\n",
              " 'irene',\n",
              " 'adler',\n",
              " 'of',\n",
              " 'dubious',\n",
              " 'and',\n",
              " 'questionable',\n",
              " 'memory',\n",
              " 'i',\n",
              " 'had',\n",
              " 'seen',\n",
              " 'little',\n",
              " 'of',\n",
              " 'holmes',\n",
              " 'lately',\n",
              " 'my',\n",
              " 'marriage',\n",
              " 'had',\n",
              " 'drifted',\n",
              " 'us',\n",
              " 'away',\n",
              " 'from',\n",
              " 'each',\n",
              " 'other',\n",
              " 'my',\n",
              " 'own',\n",
              " 'complete',\n",
              " 'happiness',\n",
              " 'and',\n",
              " 'the',\n",
              " 'home',\n",
              " 'centred',\n",
              " 'interests',\n",
              " 'which',\n",
              " 'rise',\n",
              " 'up',\n",
              " 'around',\n",
              " 'the',\n",
              " 'man',\n",
              " 'who',\n",
              " 'first',\n",
              " 'finds',\n",
              " 'himself',\n",
              " 'master',\n",
              " 'of',\n",
              " 'his',\n",
              " 'own',\n",
              " 'establishment',\n",
              " 'were',\n",
              " 'sufficient',\n",
              " 'to',\n",
              " 'absorb',\n",
              " 'all',\n",
              " 'my',\n",
              " 'attention',\n",
              " 'while',\n",
              " 'holmes',\n",
              " 'who',\n",
              " 'loathed',\n",
              " 'every',\n",
              " 'form',\n",
              " 'of',\n",
              " 'society',\n",
              " 'with',\n",
              " 'his',\n",
              " 'whole',\n",
              " 'bohemian',\n",
              " 'soul',\n",
              " 'remained',\n",
              " 'in',\n",
              " 'our',\n",
              " 'lodgings',\n",
              " 'in',\n",
              " 'baker',\n",
              " 'street',\n",
              " 'buried',\n",
              " 'among',\n",
              " 'his',\n",
              " 'old',\n",
              " 'books',\n",
              " 'and',\n",
              " 'alternating',\n",
              " 'from',\n",
              " 'week',\n",
              " 'to',\n",
              " 'week',\n",
              " 'between',\n",
              " 'cocaine',\n",
              " 'and',\n",
              " 'ambition',\n",
              " 'the',\n",
              " 'drowsiness',\n",
              " 'of',\n",
              " 'the',\n",
              " 'drug',\n",
              " 'and',\n",
              " 'the',\n",
              " 'fierce',\n",
              " 'energy',\n",
              " 'of',\n",
              " 'his',\n",
              " 'own',\n",
              " 'keen',\n",
              " 'nature',\n",
              " 'he',\n",
              " 'was',\n",
              " 'still',\n",
              " 'as',\n",
              " 'ever',\n",
              " 'deeply',\n",
              " 'attracted',\n",
              " 'by',\n",
              " 'the',\n",
              " 'study',\n",
              " 'of',\n",
              " 'crime',\n",
              " 'and',\n",
              " 'occupied',\n",
              " 'his',\n",
              " 'immense',\n",
              " 'faculties',\n",
              " 'and',\n",
              " 'extraordinary',\n",
              " 'powers',\n",
              " 'of',\n",
              " 'observation',\n",
              " 'in',\n",
              " 'following',\n",
              " 'out',\n",
              " 'those',\n",
              " 'clues',\n",
              " 'and',\n",
              " 'clearing',\n",
              " 'up',\n",
              " 'those',\n",
              " 'mysteries',\n",
              " 'which',\n",
              " 'had',\n",
              " 'been',\n",
              " 'abandoned',\n",
              " 'as',\n",
              " 'hopeless',\n",
              " 'by',\n",
              " 'the',\n",
              " 'official',\n",
              " 'police',\n",
              " 'from',\n",
              " 'time',\n",
              " 'to',\n",
              " 'time',\n",
              " 'i',\n",
              " 'heard',\n",
              " 'some',\n",
              " 'vague',\n",
              " 'account',\n",
              " 'of',\n",
              " 'his',\n",
              " 'doings',\n",
              " 'of',\n",
              " 'his',\n",
              " 'summons',\n",
              " 'to',\n",
              " 'odessa',\n",
              " 'in',\n",
              " 'the',\n",
              " 'case',\n",
              " 'of',\n",
              " 'the',\n",
              " 'trepoff',\n",
              " 'murder',\n",
              " 'of',\n",
              " 'his',\n",
              " 'clearing',\n",
              " 'up',\n",
              " 'of',\n",
              " 'the',\n",
              " 'singular',\n",
              " 'tragedy',\n",
              " 'of',\n",
              " 'the',\n",
              " 'atkinson',\n",
              " 'brothers',\n",
              " 'at',\n",
              " 'trincomalee',\n",
              " 'and',\n",
              " 'finally',\n",
              " 'of',\n",
              " 'the',\n",
              " 'mission',\n",
              " 'which',\n",
              " 'he',\n",
              " 'had',\n",
              " 'accomplished',\n",
              " 'so',\n",
              " 'delicately',\n",
              " 'and',\n",
              " 'successfully',\n",
              " 'for',\n",
              " 'the',\n",
              " 'reigning',\n",
              " 'family',\n",
              " 'of',\n",
              " 'holland',\n",
              " 'beyond',\n",
              " 'these',\n",
              " 'signs',\n",
              " 'of',\n",
              " 'his',\n",
              " 'activity',\n",
              " 'however',\n",
              " 'which',\n",
              " 'i',\n",
              " 'merely',\n",
              " 'shared',\n",
              " 'with',\n",
              " 'all',\n",
              " 'the',\n",
              " 'readers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'daily',\n",
              " 'press',\n",
              " 'i',\n",
              " 'knew',\n",
              " 'little',\n",
              " 'of',\n",
              " 'my',\n",
              " 'former',\n",
              " 'friend',\n",
              " 'and',\n",
              " 'companion',\n",
              " 'one',\n",
              " 'night',\n",
              " 'it',\n",
              " 'was',\n",
              " 'on',\n",
              " 'the',\n",
              " 'twentieth',\n",
              " 'of',\n",
              " 'march',\n",
              " '1888',\n",
              " 'i',\n",
              " 'was',\n",
              " 'returning',\n",
              " 'from',\n",
              " 'a',\n",
              " 'journey',\n",
              " 'to',\n",
              " 'a',\n",
              " 'patient',\n",
              " 'for',\n",
              " 'i',\n",
              " 'had',\n",
              " 'now',\n",
              " 'returned',\n",
              " 'to',\n",
              " 'civil',\n",
              " 'practice',\n",
              " 'when',\n",
              " 'my',\n",
              " 'way',\n",
              " 'led',\n",
              " 'me',\n",
              " 'through',\n",
              " 'baker',\n",
              " 'street',\n",
              " 'as',\n",
              " 'i',\n",
              " 'passed',\n",
              " 'the',\n",
              " 'well',\n",
              " 'remembered',\n",
              " 'door',\n",
              " 'which',\n",
              " 'must',\n",
              " 'always',\n",
              " 'be',\n",
              " 'associated',\n",
              " 'in',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'with',\n",
              " 'my',\n",
              " 'wooing',\n",
              " 'and',\n",
              " 'with',\n",
              " 'the',\n",
              " 'dark',\n",
              " 'incidents',\n",
              " 'of',\n",
              " 'the',\n",
              " 'study',\n",
              " 'in',\n",
              " 'scarlet',\n",
              " 'i',\n",
              " 'was',\n",
              " 'seized',\n",
              " 'with',\n",
              " 'a',\n",
              " 'keen',\n",
              " 'desire',\n",
              " 'to',\n",
              " 'see',\n",
              " 'holmes',\n",
              " 'again',\n",
              " 'and',\n",
              " 'to',\n",
              " 'know',\n",
              " 'how',\n",
              " 'he',\n",
              " 'was',\n",
              " 'employing',\n",
              " 'his',\n",
              " 'extraordinary',\n",
              " 'powers',\n",
              " 'his',\n",
              " 'rooms',\n",
              " 'were',\n",
              " 'brilliantly',\n",
              " 'lit',\n",
              " 'and',\n",
              " 'even',\n",
              " 'as',\n",
              " 'i',\n",
              " 'looked',\n",
              " 'up',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'his',\n",
              " 'tall',\n",
              " 'spare',\n",
              " 'figure',\n",
              " 'pass',\n",
              " 'twice',\n",
              " 'in',\n",
              " 'a',\n",
              " 'dark',\n",
              " 'silhouette',\n",
              " 'against',\n",
              " 'the',\n",
              " 'blind',\n",
              " 'he',\n",
              " 'was',\n",
              " 'pacing',\n",
              " 'the',\n",
              " 'room',\n",
              " 'swiftly',\n",
              " 'eagerly',\n",
              " 'with',\n",
              " 'his',\n",
              " 'head',\n",
              " 'sunk',\n",
              " 'upon',\n",
              " 'his',\n",
              " 'chest',\n",
              " 'and',\n",
              " 'his',\n",
              " 'hands',\n",
              " 'clasped',\n",
              " 'behind',\n",
              " 'him',\n",
              " 'to',\n",
              " 'me',\n",
              " 'who',\n",
              " 'knew',\n",
              " 'his',\n",
              " 'every',\n",
              " 'mood',\n",
              " 'and',\n",
              " 'habit',\n",
              " 'his',\n",
              " 'attitude',\n",
              " 'and',\n",
              " 'manner',\n",
              " 'told',\n",
              " 'their',\n",
              " 'own',\n",
              " 'story',\n",
              " 'he',\n",
              " 'was',\n",
              " 'at',\n",
              " 'work',\n",
              " 'again',\n",
              " 'he',\n",
              " 'had',\n",
              " 'risen',\n",
              " 'out',\n",
              " 'of',\n",
              " 'his',\n",
              " 'drug',\n",
              " 'created',\n",
              " 'dreams',\n",
              " 'and',\n",
              " 'was',\n",
              " 'hot',\n",
              " 'upon',\n",
              " 'the',\n",
              " 'scent',\n",
              " 'of',\n",
              " 'some',\n",
              " 'new',\n",
              " 'problem',\n",
              " 'i',\n",
              " 'rang',\n",
              " 'the',\n",
              " 'bell',\n",
              " 'and',\n",
              " 'was',\n",
              " 'shown',\n",
              " 'up',\n",
              " 'to',\n",
              " 'the',\n",
              " 'chamber',\n",
              " 'which',\n",
              " 'had',\n",
              " 'formerly',\n",
              " 'been',\n",
              " 'in',\n",
              " 'part',\n",
              " 'my',\n",
              " 'own',\n",
              " 'his',\n",
              " 'manner',\n",
              " 'was',\n",
              " 'not',\n",
              " 'effusive',\n",
              " 'it',\n",
              " 'seldom',\n",
              " 'was',\n",
              " 'but',\n",
              " 'he',\n",
              " 'was',\n",
              " 'glad',\n",
              " 'i',\n",
              " 'think',\n",
              " 'to',\n",
              " 'see',\n",
              " 'me',\n",
              " 'with',\n",
              " 'hardly',\n",
              " 'a',\n",
              " 'word',\n",
              " 'spoken',\n",
              " 'but',\n",
              " 'with',\n",
              " 'a',\n",
              " 'kindly',\n",
              " 'eye',\n",
              " 'he',\n",
              " 'waved',\n",
              " 'me',\n",
              " 'to',\n",
              " 'an',\n",
              " 'armchair',\n",
              " 'threw',\n",
              " 'across',\n",
              " 'his',\n",
              " 'case',\n",
              " 'of',\n",
              " 'cigars',\n",
              " 'and',\n",
              " 'indicated',\n",
              " 'a',\n",
              " 'spirit',\n",
              " 'case',\n",
              " 'and',\n",
              " 'a',\n",
              " 'gasogene',\n",
              " 'in',\n",
              " 'the',\n",
              " 'corner',\n",
              " 'then',\n",
              " 'he',\n",
              " 'stood',\n",
              " 'before',\n",
              " 'the',\n",
              " 'fire',\n",
              " 'and',\n",
              " 'looked',\n",
              " 'me',\n",
              " 'over',\n",
              " 'in',\n",
              " 'his',\n",
              " 'singular',\n",
              " 'introspective',\n",
              " 'fashion',\n",
              " 'wedlock',\n",
              " 'suits',\n",
              " 'you',\n",
              " 'he',\n",
              " 'remarked',\n",
              " 'i',\n",
              " 'think',\n",
              " 'watson',\n",
              " 'that',\n",
              " 'you',\n",
              " 'have',\n",
              " 'put',\n",
              " 'on',\n",
              " 'seven',\n",
              " 'and',\n",
              " 'a',\n",
              " 'half',\n",
              " 'pounds',\n",
              " 'since',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'you',\n",
              " 'seven',\n",
              " 'i',\n",
              " 'answered',\n",
              " 'indeed',\n",
              " 'i',\n",
              " 'should',\n",
              " 'have',\n",
              " 'thought',\n",
              " 'a',\n",
              " 'little',\n",
              " 'more',\n",
              " 'just',\n",
              " 'a',\n",
              " 'trifle',\n",
              " 'more',\n",
              " 'i',\n",
              " 'fancy',\n",
              " 'watson',\n",
              " 'and',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'again',\n",
              " 'i',\n",
              " 'observe',\n",
              " 'you',\n",
              " 'did',\n",
              " 'not',\n",
              " 'tell',\n",
              " 'me',\n",
              " 'that',\n",
              " 'you',\n",
              " 'intended',\n",
              " 'to',\n",
              " 'go',\n",
              " 'into',\n",
              " 'harness',\n",
              " 'then',\n",
              " 'how',\n",
              " 'do',\n",
              " 'you',\n",
              " 'know',\n",
              " 'i',\n",
              " 'see',\n",
              " 'it',\n",
              " 'i',\n",
              " 'deduce',\n",
              " 'it',\n",
              " 'how',\n",
              " 'do',\n",
              " 'i',\n",
              " 'know',\n",
              " 'that',\n",
              " 'you',\n",
              " 'have',\n",
              " 'been',\n",
              " 'getting',\n",
              " 'yourself',\n",
              " 'very',\n",
              " 'wet',\n",
              " 'lately',\n",
              " 'and',\n",
              " 'that',\n",
              " 'you',\n",
              " 'have',\n",
              " 'a',\n",
              " 'most',\n",
              " 'clumsy',\n",
              " 'and',\n",
              " 'careless',\n",
              " 'servant',\n",
              " 'girl',\n",
              " 'my',\n",
              " 'dear',\n",
              " 'holmes',\n",
              " 'said',\n",
              " 'i',\n",
              " 'this',\n",
              " 'is',\n",
              " 'too',\n",
              " 'much',\n",
              " 'you',\n",
              " 'would',\n",
              " 'certainly',\n",
              " 'have',\n",
              " 'been',\n",
              " 'burned',\n",
              " 'had',\n",
              " 'you',\n",
              " 'lived',\n",
              " 'a',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfcq3ltm8hDd"
      },
      "source": [
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCApaeF68tnP",
        "outputId": "1633b7be-3a1a-436a-ccc4-a1e845230d5c"
      },
      "source": [
        "#Feature Engineering\n",
        "\n",
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "print(prev_words[0])\n",
        "print(next_words[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['project', 'gutenberg', 's', 'the', 'adventures']\n",
            "of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBJBzYuJ84_l"
      },
      "source": [
        "#We will now create 2 NumPy Arrays, x for storing the features and y for storing its corresponding label. if the word is available both X and Y becomes 1\n",
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C2wsSf79TLS",
        "outputId": "4358976d-336b-4de6-e2a4-452088f1dada"
      },
      "source": [
        "print(X[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False ... False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6sjb4CA9pua"
      },
      "source": [
        "##Model Building - RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebHqadPC9hJP",
        "outputId": "ea3dca3b-c8f5-41bb-d31e-4ae06b8caeb3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               4264960   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8201)              1057929   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8201)              0         \n",
            "=================================================================\n",
            "Total params: 5,322,889\n",
            "Trainable params: 5,322,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrf9miQ4IaPu"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ERiVfq9xrj",
        "outputId": "0097b405-df45-4475-e078-dd48185dffc7"
      },
      "source": [
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=20, shuffle=True).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "811/811 [==============================] - 290s 355ms/step - loss: 5.7957 - accuracy: 0.1750 - val_loss: 8.0124 - val_accuracy: 0.1097\n",
            "Epoch 2/20\n",
            "811/811 [==============================] - 284s 350ms/step - loss: 5.4209 - accuracy: 0.2115 - val_loss: 8.2326 - val_accuracy: 0.1109\n",
            "Epoch 3/20\n",
            "811/811 [==============================] - 285s 351ms/step - loss: 5.0820 - accuracy: 0.2510 - val_loss: 8.3132 - val_accuracy: 0.1007\n",
            "Epoch 4/20\n",
            "811/811 [==============================] - 278s 343ms/step - loss: 4.8100 - accuracy: 0.2895 - val_loss: 8.4893 - val_accuracy: 0.0879\n",
            "Epoch 5/20\n",
            "811/811 [==============================] - 281s 347ms/step - loss: 4.5698 - accuracy: 0.3285 - val_loss: 8.8796 - val_accuracy: 0.0882\n",
            "Epoch 6/20\n",
            "811/811 [==============================] - 278s 343ms/step - loss: 4.3484 - accuracy: 0.3687 - val_loss: 8.7436 - val_accuracy: 0.0817\n",
            "Epoch 7/20\n",
            "811/811 [==============================] - 278s 342ms/step - loss: 4.1733 - accuracy: 0.4040 - val_loss: 9.1702 - val_accuracy: 0.0813\n",
            "Epoch 8/20\n",
            "811/811 [==============================] - 285s 352ms/step - loss: 4.0331 - accuracy: 0.4353 - val_loss: 9.4321 - val_accuracy: 0.0771\n",
            "Epoch 9/20\n",
            "811/811 [==============================] - 280s 345ms/step - loss: 3.9188 - accuracy: 0.4649 - val_loss: 9.3898 - val_accuracy: 0.0747\n",
            "Epoch 10/20\n",
            "811/811 [==============================] - 284s 350ms/step - loss: 3.8047 - accuracy: 0.4891 - val_loss: 9.4799 - val_accuracy: 0.0703\n",
            "Epoch 11/20\n",
            "811/811 [==============================] - 282s 348ms/step - loss: 3.7086 - accuracy: 0.5098 - val_loss: 9.9142 - val_accuracy: 0.0690\n",
            "Epoch 12/20\n",
            "811/811 [==============================] - 281s 347ms/step - loss: 3.6276 - accuracy: 0.5290 - val_loss: 9.9170 - val_accuracy: 0.0654\n",
            "Epoch 13/20\n",
            "811/811 [==============================] - 277s 342ms/step - loss: 3.5867 - accuracy: 0.5420 - val_loss: 10.2020 - val_accuracy: 0.0683\n",
            "Epoch 14/20\n",
            "811/811 [==============================] - 281s 347ms/step - loss: 3.5232 - accuracy: 0.5570 - val_loss: 10.2292 - val_accuracy: 0.0663\n",
            "Epoch 15/20\n",
            "811/811 [==============================] - 279s 344ms/step - loss: 3.4458 - accuracy: 0.5702 - val_loss: 10.4076 - val_accuracy: 0.0668\n",
            "Epoch 16/20\n",
            "811/811 [==============================] - 281s 346ms/step - loss: 3.3925 - accuracy: 0.5809 - val_loss: 10.5575 - val_accuracy: 0.0648\n",
            "Epoch 17/20\n",
            "811/811 [==============================] - 277s 342ms/step - loss: 3.3505 - accuracy: 0.5923 - val_loss: 10.5619 - val_accuracy: 0.0652\n",
            "Epoch 18/20\n",
            "811/811 [==============================] - 277s 342ms/step - loss: 3.3281 - accuracy: 0.5977 - val_loss: 10.7278 - val_accuracy: 0.0611\n",
            "Epoch 19/20\n",
            "811/811 [==============================] - 278s 343ms/step - loss: 3.2860 - accuracy: 0.6069 - val_loss: 11.0303 - val_accuracy: 0.0602\n",
            "Epoch 20/20\n",
            "811/811 [==============================] - 277s 341ms/step - loss: 3.2408 - accuracy: 0.6144 - val_loss: 11.1046 - val_accuracy: 0.0584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epNv_U_iIgqw"
      },
      "source": [
        "#Sucessfully trained our model, now we are saving it\n",
        "model.save('keras_next_word_model.h5')\n",
        "\n",
        "model = load_model('keras_next_word_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s74KUBThr9sR"
      },
      "source": [
        "#Testing Next Word Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi7APnuTqncM",
        "outputId": "90bfca9c-09f8-4c04-a758-3ad139ec6202"
      },
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        print(word)\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"It is not a lack\".lower())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it\n",
            "is\n",
            "not\n",
            "a\n",
            "lack\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TFb6aBe_TY21",
        "outputId": "bea1b429-8ae7-4f02-a016-5d386260d303"
      },
      "source": [
        "\n",
        "unique_words[3010]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for'"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-eRh0NYsTzp",
        "outputId": "2cc26671-64b6-402d-f572-9c74ad05292a"
      },
      "source": [
        "txt = input('Enter text :')\n",
        "\n",
        "prepare_input(txt.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter text :he turned hungrily on\n",
            "he\n",
            "turned\n",
            "hungrily\n",
            "on\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrev8EfMtRQ6",
        "outputId": "138fe5ed-88b9-48ea-b3d6-74ea7346b3b5"
      },
      "source": [
        "txt.split()[-4:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'turned', 'hungrily', 'on']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33gbFSMWsvcT"
      },
      "source": [
        "def pediction(model,tokenize,txt):\n",
        "    sequence1 = tokenize.tokenize(txt)\n",
        "    sequence = np.array(sequence1)\n",
        "    print(sequence1)\n",
        "    preds= np.argmax(model.predict(sequence1))\n",
        "    pred_wods=\"\"\n",
        "    print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0djSA_BR8Ou",
        "outputId": "b0afb8ea-915a-47d3-9d97-effc562fd3e7"
      },
      "source": [
        "prepare_input('she Sherlock Holmes she'.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "she\n",
            "sherlock\n",
            "holmes\n",
            "she\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FCZemo6UTufr",
        "outputId": "9a8883cf-ae7c-45d8-bb85-e5e33493ab32"
      },
      "source": [
        "unique_words[11]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12_s_'"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikH7HoJ3Tz_3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}